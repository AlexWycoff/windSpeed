{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b73532717ceab7c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Model Analysis Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89411512a29a662",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "In this notebook, we train, test, and evaluate the performance of an LSTM model in wind speed prediction and compare results to the persistence method, which is a common benchmark for wind speed prediction algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Input, LSTM, Dense, Dropout, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482b2cba580d03b1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Define how many time steps will be used in observation and prediction\n",
    "n_past = 24 # The last day of data\n",
    "n_future = 24 # The next day of data\n",
    "n_features = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db83f8ff572993e6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Define a function to split the series using a sliding window\n",
    "def split_series(series, n_past=n_past, n_future=n_future, offset=0):\n",
    "    X, y = list(), list()\n",
    "    for i in range(int(len(series)/n_past)-1):\n",
    "        X.append(series[i*n_past : i*n_past + n_past, :])\n",
    "        y.append(series[offset + i*n_past + n_past : offset + i*n_past + n_past + n_future, :])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbda487b9b141fb5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Process and split the data for a site given its filename\n",
    "def prep_data(filename):\n",
    "    # Import the data for a single point\n",
    "    data = pd.read_csv(\"Data/NOW-23 Great Lakes [2000-2020] 60min/\" + filename, index_col=0)\n",
    "\n",
    "    # Restrict the data to the last 5 years, giving us 4 years of training and 1 year of testing data\n",
    "    data = data.iloc[int(len(data)*1/4):]\n",
    "\n",
    "    # Split the data into training and testing samples\n",
    "    cutoff = int(len(data)*0.8)\n",
    "    test_data = data[cutoff:]\n",
    "    data = data[:cutoff]\n",
    "    \n",
    "    # Normalize the testing and training data\n",
    "    test_data.iloc[:, 6:9], test_norms = normalize(test_data.iloc[:, 6:9], axis=0, norm='max', return_norm=True)\n",
    "    data.iloc[:, 6:9], train_norms = normalize(data.iloc[:, 6:9], axis=0, norm='max', return_norm=True)\n",
    "\n",
    "    # Split the data into series for training\n",
    "    X_train, y_train = split_series(np.array(data.iloc[:, 6:9]), n_future=1, offset=24-1)\n",
    "    X_test, y_test = split_series(np.array(test_data.iloc[:, 6:9]), n_future=1, offset=24-1)\n",
    "\n",
    "    # Adjust the expected output to contain only the wind speed\n",
    "    y_train, y_test = y_train[:, :, 2], y_test[:, :, 2]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, train_norms, test_norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1c2254f90f8a8c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Define the model architecture\n",
    "def define_model():\n",
    "    # Original model used for testing\n",
    "    '''\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(Input(shape=(n_past, n_features)))\n",
    "    model.add(LSTM(128, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(128, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)))\n",
    "    model.add(Dense(1, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)))\n",
    "    model.compile(optimizer='adam', loss='mae')\n",
    "    '''\n",
    "    \n",
    "    # Lightweight model used for additional training\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(Input(shape=(n_past, n_features)))\n",
    "    model.add(LSTM(16, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(8, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)))\n",
    "    model.add(Dense(1, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)))\n",
    "    model.compile(optimizer='adam', loss='mae')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6343d8cad2878f10",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model = define_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b9849809ad1fda",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "i = 1\n",
    "for filename in os.listdir(\"Data/NOW-23 Great Lakes [2000-2020] 60min\"):\n",
    "    print(f\"Point number {i} of 100\")\n",
    "    i += 1\n",
    "\n",
    "    model = define_model()\n",
    "    \n",
    "    X_train, y_train, X_test, y_test, train_norms, test_norms = prep_data(filename)\n",
    "    \n",
    "    model.fit(X_train,y_train,epochs=100,validation_data=(X_test,y_test),batch_size=128)\n",
    "    # model.save('Data/Models/' + filename[:-4] + '.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51a9d64c64daf62",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "mae, sites = list(), list()\n",
    "\n",
    "for filename in os.listdir(\"Data/Models\"):\n",
    "\n",
    "    X_train, y_train, X_test, y_test, train_norms, test_norms = prep_data(filename[:-6] + '.csv')\n",
    "    model = keras.saving.load_model(\"Data/Models/\" + filename)\n",
    "    model.compile(optimizer='adam', loss='mae')\n",
    "    \n",
    "    predictions = model.predict(X_test)\n",
    "    mae.append(mean_absolute_error(y_test[:, 0] * test_norms[2], predictions * test_norms[2]))\n",
    "    sites.append(filename[:-6])\n",
    "    \n",
    "df = pd.DataFrame()\n",
    "df['MAE'] = pd.Series(mae)\n",
    "df['SiteID'] = pd.Series(sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e04a583edf7d6f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "print(plt.style.available)\n",
    "plt.style.use('seaborn-v0_8-notebook')\n",
    "plt.boxplot(df['MAE'], vert=False)\n",
    "plt.title(\"Mean Absolute Error\")\n",
    "plt.show()\n",
    "\n",
    "df['MAE'].hist(bins=20)\n",
    "plt.title(\"Mean Absolute Error\")\n",
    "plt.show()\n",
    "\n",
    "sns.kdeplot(df['MAE'], color=\"blue\", fill=True)\n",
    "plt.title(\"Mean Absolute Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43f5c15043c51b3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Mean: {np.average(df['MAE'])}\")\n",
    "print(f\"Median: {np.median(df['MAE'])}\")\n",
    "print(f\"Standard Deviation: {np.std(df['MAE'])}\")\n",
    "print(f\"n: {len(df['MAE'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29331c2c6d132ff6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Finally, we repeat this analysis with a persistence model that uses the wind speed from 24h before as a prediction, demonstrating the superiority of the LSTM model\n",
    "\n",
    "mae, sites = list(), list()\n",
    "\n",
    "for filename in os.listdir(\"Data/Models\"):\n",
    "\n",
    "    X_train, y_train, X_test, y_test, train_norms, test_norms = prep_data(filename[:-6] + '.csv')\n",
    "\n",
    "    predictions = [x[-1] for x in X_test[:, :, -1]]\n",
    "    mae.append(mean_absolute_error(y_test[:, 0] * test_norms[2], np.array(predictions) * test_norms[2]))\n",
    "    sites.append(filename[:-6])\n",
    "\n",
    "df1 = pd.DataFrame()\n",
    "df1['MAE'] = pd.Series(mae)\n",
    "df1['SiteID'] = pd.Series(sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8163a5da2b274f5f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df1['MAE'].hist(bins=20)\n",
    "plt.show()\n",
    "\n",
    "sns.kdeplot(df1['MAE'], color=\"blue\", fill=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c100b6dca60797aa",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "total = 0\n",
    "for i in df1['MAE']:\n",
    "    total += i\n",
    "peristence_avg_mae = total/len(df1['MAE'])\n",
    "print(f\"Average MAE of the persistence model: {peristence_avg_mae}\")\n",
    "print(f\"The persistence model has a higher MAE by {(peristence_avg_mae/np.average(df['MAE']) - 1) * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa914670d8106f48",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Unsurpisingly, the average MAE of the persistence model is around 32% higher than the average LSTM model MAE over all sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c3f993575a494b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model = keras.saving.load_model(\"Data/Models/\" + '8096.keras')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc11968783d3b2e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
