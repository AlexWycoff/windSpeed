{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b73532717ceab7c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Model Analysis Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89411512a29a662",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In this notebook, we train, test, and evaluate the performance of an LSTM model in wind speed prediction and compare results to the persistence method, which is a common benchmark for wind speed prediction algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Input, LSTM, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482b2cba580d03b1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define how many time steps will be used in observation and prediction\n",
    "n_past = 24 # The last day of data\n",
    "n_future = 24 # The next day of data\n",
    "n_features = 3\n",
    "\n",
    "# Set the universal font size for matplotlib\n",
    "plt.rcParams['font.size'] = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db83f8ff572993e6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a function to split the series using a sliding window\n",
    "def split_series(series, n_past=n_past, n_future=n_future, offset=0):\n",
    "    X, y = list(), list()\n",
    "    for i in range(int(len(series)/n_past)-1):\n",
    "        X.append(series[i*n_past : i*n_past + n_past, :])\n",
    "        y.append(series[offset + i*n_past + n_past : offset + i*n_past + n_past + n_future, :])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbda487b9b141fb5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Process and split the data for a site given its filename\n",
    "def prep_data(filename, cy=2015):\n",
    "    # Import the data for a single point\n",
    "    data = pd.read_csv(\"Data/NOW-23 Great Lakes [2000-2020] 60min/\" + filename, index_col=0)\n",
    "\n",
    "    # Restrict the data to the last 5 years, giving us 4 years of training and 1 year of testing data\n",
    "    data = data.iloc[int(len(data)*(cy-2000)/20):]\n",
    "\n",
    "    # Split the data into training and testing samples\n",
    "    cutoff = int(len(data)*0.8)\n",
    "    test_data = data[cutoff:]\n",
    "    data = data[:cutoff]\n",
    "    \n",
    "    # Designate which columns are used for training\n",
    "    columns = [6, 7, 8]\n",
    "    \n",
    "    # Normalize the testing and training data\n",
    "    test_data.iloc[:, columns], test_norms = normalize(test_data.iloc[:, columns], axis=0, norm='max', return_norm=True)\n",
    "    data.iloc[:, columns], train_norms = normalize(data.iloc[:, columns], axis=0, norm='max', return_norm=True)\n",
    "\n",
    "    # Split the data into series for training\n",
    "    X_train, y_train = split_series(np.array(data.iloc[:, columns]), n_future=1, offset=24-1)\n",
    "    X_test, y_test = split_series(np.array(test_data.iloc[:, columns]), n_future=1, offset=24-1)\n",
    "\n",
    "    # Adjust the expected output to contain only the wind speed\n",
    "    y_train, y_test = y_train[:, :, 2], y_test[:, :, 2]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, train_norms, test_norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6ecadfc9437cad",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the model architecture\n",
    "def define_model():\n",
    "    # Original model used for testing\n",
    "    '''\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(Input(shape=(n_past, n_features)))\n",
    "    model.add(LSTM(128, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(128, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)))\n",
    "    model.add(Dense(1, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)))\n",
    "    model.compile(optimizer='adam', loss='mae')\n",
    "    '''\n",
    "\n",
    "\n",
    "    # Lighter model used for additional training\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(Input(shape=(n_past, n_features)))\n",
    "    model.add(LSTM(16, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(8, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)))\n",
    "    model.add(Dense(1, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)))\n",
    "    model.compile(optimizer='adam', loss='mae')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6343d8cad2878f10",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = define_model()\n",
    "model.summary()\n",
    "model.input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edcd7e806169f1a8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 10\u001B[0m\n\u001B[0;32m      8\u001B[0m X_train, y_train, X_test, y_test, train_norms, test_norms \u001B[38;5;241m=\u001B[39m prep_data(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m7871.csv\u001B[39m\u001B[38;5;124m'\u001B[39m, cy\u001B[38;5;241m=\u001B[39myear)\n\u001B[0;32m      9\u001B[0m model \u001B[38;5;241m=\u001B[39m define_model()\n\u001B[1;32m---> 10\u001B[0m model\u001B[38;5;241m.\u001B[39mfit(X_train, y_train, epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m, validation_data\u001B[38;5;241m=\u001B[39m(X_test, y_test), batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m128\u001B[39m)\n\u001B[0;32m     12\u001B[0m predictions \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpredict(X_test)\n\u001B[0;32m     13\u001B[0m mae \u001B[38;5;241m=\u001B[39m mean_absolute_error(y_test[:, \u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m*\u001B[39m test_norms[\u001B[38;5;241m2\u001B[39m], predictions \u001B[38;5;241m*\u001B[39m train_norms[\u001B[38;5;241m2\u001B[39m])\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    115\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    116\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 117\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    118\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    119\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:318\u001B[0m, in \u001B[0;36mTensorFlowTrainer.fit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001B[0m\n\u001B[0;32m    316\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m step, iterator \u001B[38;5;129;01min\u001B[39;00m epoch_iterator\u001B[38;5;241m.\u001B[39menumerate_epoch():\n\u001B[0;32m    317\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_begin(step)\n\u001B[1;32m--> 318\u001B[0m     logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_function(iterator)\n\u001B[0;32m    319\u001B[0m     logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pythonify_logs(logs)\n\u001B[0;32m    320\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_end(step, logs)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    830\u001B[0m compiler \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnonXla\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    832\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile):\n\u001B[1;32m--> 833\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m    835\u001B[0m new_tracing_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_get_tracing_count()\n\u001B[0;32m    836\u001B[0m without_tracing \u001B[38;5;241m=\u001B[39m (tracing_count \u001B[38;5;241m==\u001B[39m new_tracing_count)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001B[0m, in \u001B[0;36mFunction._call\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    875\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[0;32m    876\u001B[0m \u001B[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001B[39;00m\n\u001B[0;32m    877\u001B[0m \u001B[38;5;66;03m# run the first trace but we should fail if variables are created.\u001B[39;00m\n\u001B[1;32m--> 878\u001B[0m results \u001B[38;5;241m=\u001B[39m tracing_compilation\u001B[38;5;241m.\u001B[39mcall_function(\n\u001B[0;32m    879\u001B[0m     args, kwds, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_variable_creation_config\n\u001B[0;32m    880\u001B[0m )\n\u001B[0;32m    881\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_created_variables:\n\u001B[0;32m    882\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCreating variables on a non-first call to a function\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    883\u001B[0m                    \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m decorated with tf.function.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001B[0m, in \u001B[0;36mcall_function\u001B[1;34m(args, kwargs, tracing_options)\u001B[0m\n\u001B[0;32m    137\u001B[0m bound_args \u001B[38;5;241m=\u001B[39m function\u001B[38;5;241m.\u001B[39mfunction_type\u001B[38;5;241m.\u001B[39mbind(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    138\u001B[0m flat_inputs \u001B[38;5;241m=\u001B[39m function\u001B[38;5;241m.\u001B[39mfunction_type\u001B[38;5;241m.\u001B[39munpack_inputs(bound_args)\n\u001B[1;32m--> 139\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m function\u001B[38;5;241m.\u001B[39m_call_flat(  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[0;32m    140\u001B[0m     flat_inputs, captured_inputs\u001B[38;5;241m=\u001B[39mfunction\u001B[38;5;241m.\u001B[39mcaptured_inputs\n\u001B[0;32m    141\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001B[0m, in \u001B[0;36mConcreteFunction._call_flat\u001B[1;34m(self, tensor_inputs, captured_inputs)\u001B[0m\n\u001B[0;32m   1318\u001B[0m possible_gradient_type \u001B[38;5;241m=\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPossibleTapeGradientTypes(args)\n\u001B[0;32m   1319\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (possible_gradient_type \u001B[38;5;241m==\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001B[0;32m   1320\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m executing_eagerly):\n\u001B[0;32m   1321\u001B[0m   \u001B[38;5;66;03m# No tape is watching; skip to running the function.\u001B[39;00m\n\u001B[1;32m-> 1322\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inference_function\u001B[38;5;241m.\u001B[39mcall_preflattened(args)\n\u001B[0;32m   1323\u001B[0m forward_backward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_select_forward_and_backward_functions(\n\u001B[0;32m   1324\u001B[0m     args,\n\u001B[0;32m   1325\u001B[0m     possible_gradient_type,\n\u001B[0;32m   1326\u001B[0m     executing_eagerly)\n\u001B[0;32m   1327\u001B[0m forward_function, args_with_tangents \u001B[38;5;241m=\u001B[39m forward_backward\u001B[38;5;241m.\u001B[39mforward()\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001B[0m, in \u001B[0;36mAtomicFunction.call_preflattened\u001B[1;34m(self, args)\u001B[0m\n\u001B[0;32m    214\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcall_preflattened\u001B[39m(\u001B[38;5;28mself\u001B[39m, args: Sequence[core\u001B[38;5;241m.\u001B[39mTensor]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[0;32m    215\u001B[0m \u001B[38;5;250m  \u001B[39m\u001B[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 216\u001B[0m   flat_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcall_flat(\u001B[38;5;241m*\u001B[39margs)\n\u001B[0;32m    217\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunction_type\u001B[38;5;241m.\u001B[39mpack_output(flat_outputs)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001B[0m, in \u001B[0;36mAtomicFunction.call_flat\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m    249\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m record\u001B[38;5;241m.\u001B[39mstop_recording():\n\u001B[0;32m    250\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_bound_context\u001B[38;5;241m.\u001B[39mexecuting_eagerly():\n\u001B[1;32m--> 251\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_bound_context\u001B[38;5;241m.\u001B[39mcall_function(\n\u001B[0;32m    252\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname,\n\u001B[0;32m    253\u001B[0m         \u001B[38;5;28mlist\u001B[39m(args),\n\u001B[0;32m    254\u001B[0m         \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunction_type\u001B[38;5;241m.\u001B[39mflat_outputs),\n\u001B[0;32m    255\u001B[0m     )\n\u001B[0;32m    256\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    257\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m make_call_op_in_graph(\n\u001B[0;32m    258\u001B[0m         \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    259\u001B[0m         \u001B[38;5;28mlist\u001B[39m(args),\n\u001B[0;32m    260\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_bound_context\u001B[38;5;241m.\u001B[39mfunction_call_options\u001B[38;5;241m.\u001B[39mas_attrs(),\n\u001B[0;32m    261\u001B[0m     )\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1552\u001B[0m, in \u001B[0;36mContext.call_function\u001B[1;34m(self, name, tensor_inputs, num_outputs)\u001B[0m\n\u001B[0;32m   1550\u001B[0m cancellation_context \u001B[38;5;241m=\u001B[39m cancellation\u001B[38;5;241m.\u001B[39mcontext()\n\u001B[0;32m   1551\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cancellation_context \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1552\u001B[0m   outputs \u001B[38;5;241m=\u001B[39m execute\u001B[38;5;241m.\u001B[39mexecute(\n\u001B[0;32m   1553\u001B[0m       name\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m   1554\u001B[0m       num_outputs\u001B[38;5;241m=\u001B[39mnum_outputs,\n\u001B[0;32m   1555\u001B[0m       inputs\u001B[38;5;241m=\u001B[39mtensor_inputs,\n\u001B[0;32m   1556\u001B[0m       attrs\u001B[38;5;241m=\u001B[39mattrs,\n\u001B[0;32m   1557\u001B[0m       ctx\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   1558\u001B[0m   )\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1560\u001B[0m   outputs \u001B[38;5;241m=\u001B[39m execute\u001B[38;5;241m.\u001B[39mexecute_with_cancellation(\n\u001B[0;32m   1561\u001B[0m       name\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m   1562\u001B[0m       num_outputs\u001B[38;5;241m=\u001B[39mnum_outputs,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1566\u001B[0m       cancellation_manager\u001B[38;5;241m=\u001B[39mcancellation_context,\n\u001B[0;32m   1567\u001B[0m   )\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001B[0m, in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     51\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     52\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[1;32m---> 53\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m pywrap_tfe\u001B[38;5;241m.\u001B[39mTFE_Py_Execute(ctx\u001B[38;5;241m.\u001B[39m_handle, device_name, op_name,\n\u001B[0;32m     54\u001B[0m                                       inputs, attrs, num_outputs)\n\u001B[0;32m     55\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     56\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Train one model with data starting at different years\n",
    "df = pd.DataFrame()\n",
    "df['Years'] = list()\n",
    "df['MAE'] = list()\n",
    "for year in range(2000, 2020):\n",
    "    print(f\"{year}\")\n",
    "\n",
    "    X_train, y_train, X_test, y_test, train_norms, test_norms = prep_data('7871.csv', cy=year)\n",
    "    model = define_model()\n",
    "    model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test), batch_size=128)\n",
    "\n",
    "    predictions = model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test[:, 0] * test_norms[2], predictions * train_norms[2])\n",
    "\n",
    "    df.loc[len(df) + 1] = [int(year), mae]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861bcd17d03a0dc5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'Data\\Raw Experiment Data\\starting year experiment.csv')\n",
    "df['Years'] = [2020 - x for x in df['Years']]\n",
    "# Display the data for use as a figure\n",
    "ax = plt.axes()\n",
    "ax.set_facecolor(\"white\")\n",
    "plt.xlabel('Years of Data')\n",
    "plt.ylabel('Mean Absolute Error (m/s)')\n",
    "plt.xlim(-2, 22)\n",
    "plt.scatter(df['Years'], df['MAE'], color='skyblue')\n",
    "y_lim = ax.get_ylim()\n",
    "plt.grid(True, axis='y', color='grey')\n",
    "\n",
    "# Plot the polynomial line of best fit\n",
    "def plot_best_fit(x, y, deg):\n",
    "    coeffs = np.polyfit(x, y, deg)\n",
    "    \n",
    "    ylist = list()\n",
    "    for n in x:\n",
    "        yvalue = np.sum([coeffs[i]*n**(len(coeffs)-1-i) for i in range(len(coeffs))])\n",
    "        ylist.append(yvalue)\n",
    "    plt.plot(x, ylist, color='skyblue')\n",
    "\n",
    "# plot_best_fit(df['Years'], df['MAE'], 3)\n",
    "\n",
    "# Plot the logarithmic line of best fit\n",
    "def plot_log_fit(x, y, deg):\n",
    "    coeffs = np.polyfit(np.log(x), y, deg)\n",
    "\n",
    "    ylist = list()\n",
    "    for n in np.log(x):\n",
    "        yvalue = np.sum([coeffs[i]*n**(len(coeffs)-1-i) for i in range(len(coeffs))])\n",
    "        ylist.append(yvalue)\n",
    "    plt.plot(x, ylist, color='skyblue')\n",
    "\n",
    "plot_log_fit(df['Years'], df['MAE'], 3)\n",
    "\n",
    "# Remove the border from the graph\n",
    "for direction in ['top', 'right', 'bottom', 'left']:\n",
    "    ax.spines[direction].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2650c1c0f9da24",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train one model with a varying number of epochs\n",
    "df = pd.DataFrame()\n",
    "df['Epochs'] = list()\n",
    "df['MAE'] = list()\n",
    "for epoch in range(10, 160, 10):\n",
    "    print(f\"{epoch}\")\n",
    "\n",
    "    X_train, y_train, X_test, y_test, train_norms, test_norms = prep_data('7871.csv')\n",
    "    model = define_model()\n",
    "    model.fit(X_train,y_train,epochs=epoch,validation_data=(X_test,y_test),batch_size=128)\n",
    "        \n",
    "    predictions = model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test[:, 0] * test_norms[2], predictions * train_norms[2])\n",
    "        \n",
    "    df.loc[len(df)+1] = [int(epoch), mae]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b14b9e73fe7e0b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'Data\\Raw Experiment Data\\epoch experiment.csv')\n",
    "# Display the data for use as a figure\n",
    "ax = plt.axes()\n",
    "ax.set_facecolor(\"white\")\n",
    "plt.xlabel('Epochs Trained')\n",
    "plt.ylabel('Mean Absolute Error (m/s)')\n",
    "plt.ylim(y_lim)\n",
    "plt.scatter(df['Epochs'], df['MAE'], color='skyblue')\n",
    "plt.grid(True, axis='y', color='grey')\n",
    "\n",
    "# Plot the line of best fit\n",
    "# plot_best_fit(df['Epochs'], df['MAE'], 3)\n",
    "plot_log_fit(df['Epochs'], df['MAE'], 1)\n",
    "\n",
    "# Remove the border from the graph\n",
    "for direction in ['top', 'right', 'bottom', 'left']:\n",
    "    ax.spines[direction].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b9849809ad1fda",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train models for every selected site\n",
    "i = 1\n",
    "for filename in os.listdir(\"Data/NOW-23 Great Lakes [2000-2020] 60min\"):\n",
    "    print(f\"Point number {i} of 100\")\n",
    "    i += 1\n",
    "\n",
    "    model = define_model()\n",
    "    \n",
    "    X_train, y_train, X_test, y_test, train_norms, test_norms = prep_data(filename, cy=2015)\n",
    "    \n",
    "    model.fit(X_train,y_train,epochs=50,validation_data=(X_test,y_test),batch_size=128)\n",
    "    model.save(\"Data/Models/\" + filename[:-4] + \".keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51a9d64c64daf62",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test models for every selected site\n",
    "mae, sites = list(), list()\n",
    "\n",
    "for filename in os.listdir(\"Data/Models\"):\n",
    "\n",
    "    X_train, y_train, X_test, y_test, train_norms, test_norms = prep_data(filename[:-6] + '.csv', cy=2015)\n",
    "    model = keras.saving.load_model(\"Data/Models/\" + filename)\n",
    "    model.compile(optimizer='adam', loss='mae')\n",
    "    \n",
    "    predictions = model.predict(X_test)\n",
    "    mae.append(mean_absolute_error(y_test[:, 0] * test_norms[2], predictions * train_norms[2]))\n",
    "    sites.append(filename[:-6])\n",
    "    print(mae[-1])\n",
    "df = pd.DataFrame()\n",
    "df['MAE'] = pd.Series(mae)\n",
    "df['SiteID'] = pd.Series(sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29331c2c6d132ff6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Finally, we repeat this analysis with a persistence model that uses the wind speed from 24h before as a prediction, demonstrating the superiority of the LSTM model\n",
    "\n",
    "mae, rmse, sites = list(), list(), list()\n",
    "\n",
    "for filename in os.listdir(\"Data/Models\"):\n",
    "\n",
    "    X_train, y_train, X_test, y_test, train_norms, test_norms = prep_data(filename[:-6] + '.csv', cy=2015)\n",
    "\n",
    "    predictions = [x[-1] for x in X_test[:, :, -1]]\n",
    "    mae.append(mean_absolute_error(y_test[:, 0] * test_norms[2], np.array(predictions) * test_norms[2]))\n",
    "    rmse.append(np.sqrt(mean_squared_error(y_test[:, 0] * test_norms[2], np.array(predictions) * test_norms[2])))\n",
    "    sites.append(filename[:-6])\n",
    "\n",
    "df1 = pd.DataFrame()\n",
    "df1['MAE'] = pd.Series(mae)\n",
    "df1['RMSE'] = pd.Series(rmse)\n",
    "df1['SiteID'] = pd.Series(sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c100b6dca60797aa",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We compare the persistence model on a variety of loss metrics\n",
    "print(f\"Average MAE of the persistence model: {np.average(df1['MAE'])}\")\n",
    "print(f\"Median MAE of the persistence model: {np.median(df1['MAE'])}\")\n",
    "print(f\"Std MAE of the persistence model: {np.std(df1['MAE'])}\")\n",
    "print(f\"Average RMSE of the persistence model: {np.average(df1['RMSE'])}\")\n",
    "print(f\"Median RMSE of the persistence model: {np.median(df1['RMSE'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'Data\\Raw Experiment Data\\Final Model Performance.csv')\n",
    "\n",
    "# Generate a box plot to describe the MAE distribution for use as a figure\n",
    "ax = plt.axes()\n",
    "ax.set_facecolor(\"white\")\n",
    "fig = plt.gcf()\n",
    "frame = plt.gca()\n",
    "fig.set_size_inches(12, 6)\n",
    "frame.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.style.use('seaborn-v0_8-ticks')\n",
    "bplot = plt.boxplot([df['MAE'], df1['MAE']], vert=False, patch_artist=True, medianprops=dict(color='black'),\n",
    "            boxprops=dict(facecolor='white'), widths=0.25)\n",
    "\n",
    "for patch, color in zip(bplot['boxes'], ['skyblue', 'seagreen']):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "plt.xlabel(\"Mean Absolute Error (m/s)\")\n",
    "plt.tick_params(axis='x', which='major', reset=True, direction='in', top=False)\n",
    "ax.legend([bplot['boxes'][0], bplot['boxes'][1]], ['LSTM model', 'Persistence model'], loc='upper center')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec63d005f23bdfaa",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# We can print out some statistics of the distribution in detail\n",
    "print(f\"Mean: {np.average(df['MAE'])}\")\n",
    "print(f\"Median: {np.median(df['MAE'])}\")\n",
    "print(f\"Standard Deviation: {np.std(df['MAE'])}\")\n",
    "print(f\"n: {len(df['MAE'])}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f\"The persistence model has a higher MAE by {(np.average(df1['MAE'])/np.average(df['MAE']) - 1) * 100}%\")\n",
    "print(f\"Median difference: {(np.median(df1['MAE'])/np.median(df['MAE']) - 1) * 100}%\")\n",
    "# Unsurpisingly, the both the median and average MAE of the persistence model are around 25-30% higher than the average LSTM model MAE over all sites"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e479075149f92bed",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Parameters we need for statistical inference\n",
    "diff = pd.DataFrame()\n",
    "diff['MAE'] = df1['MAE'] - df['MAE']\n",
    "print('Difference statistics')\n",
    "print(f\"Mean: {np.average(diff['MAE'])}\")\n",
    "print(f\"Median: {np.median(diff['MAE'])}\")\n",
    "print(f\"Standard Deviation: {np.std(diff['MAE'])}\")\n",
    "print(f\"n: {len(diff['MAE'])}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d89cd0e7d8d966cd",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63e1cc26f7ca134",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To describe the differences in train time with different model parameters, we train 3 models, each encompassing the 100 selected points in the study.\n",
    "df = pd.DataFrame()\n",
    "df['Average MAE'] = list()\n",
    "df['Median MAE'] = list()\n",
    "df['Average RMSE'] = list()\n",
    "df['Median RMSE'] = list()\n",
    "df['Train_time'] = list()\n",
    "df['Train_time std'] = list()\n",
    "\n",
    "for pair in [[100, 2000], [100, 2015], [50, 2015]]:\n",
    "    # Train models for every selected site\n",
    "\n",
    "    mae, rmse, time_elapsed = list(), list(), list()\n",
    "    i = 1\n",
    "    for filename in os.listdir(\"Data/NOW-23 Great Lakes [2000-2020] 60min\"):\n",
    "        print(f\"Point number {i} of 100\")\n",
    "        i += 1\n",
    "        \n",
    "        start_time = datetime.now()\n",
    "        model = define_model()\n",
    "        X_train, y_train, X_test, y_test, train_norms, test_norms = prep_data(filename, cy=pair[1])\n",
    "        model.fit(X_train,y_train,epochs=pair[0],validation_data=(X_test,y_test),batch_size=128)\n",
    "\n",
    "        predictions = model.predict(X_test)\n",
    "        mae.append(mean_absolute_error(y_test[:, 0] * test_norms[2], np.array(predictions) * train_norms[2]))\n",
    "        rmse.append(np.sqrt(mean_squared_error(y_test[:, 0] * test_norms[2], np.array(predictions) * train_norms[2])))\n",
    "    \n",
    "        time_elapsed.append((datetime.now() - start_time).total_seconds())\n",
    "    df.loc[len(df)+1] = [np.average(mae), np.median(mae), np.average(rmse), np.median(rmse), np.average(time_elapsed), np.std(time_elapsed)]\n",
    "df['Model'] = ['100 epochs, 20 years', '100 epochs, 5 years', '50 epochs, 5 years']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a34e8b15485d8ab",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Data/Raw Experiment Data/Train Time Experiment.csv\")\n",
    "\n",
    "# Now we can generate a figure to demonstrate the significant change in train time between the models\n",
    "ax = plt.axes()\n",
    "ax.set_facecolor(\"white\")\n",
    "ax.set_yticks(range(5, 40, 5))\n",
    "plt.grid(True, axis='y', color='grey')\n",
    "plt.bar(df['Model'], df['Train_time'], width=0.4, color=\"skyblue\")\n",
    "plt.errorbar(df['Model'], df['Train_time'], yerr=df['Train_time std'], capsize=3, linestyle=\"\", color='deepskyblue')\n",
    "plt.ylabel(\"Average train time (seconds)\")\n",
    "plt.xlabel(\"Network parameters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3175c7c6f159a3c8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We also compare the models to persistence, finding no major differences between the models in performance but significant improvements over persistence\n",
    "df.loc[len(df)+1] = [np.average(df1['MAE']), np.median(df1['MAE']), np.average(df1['RMSE']), np.median(df1['RMSE']), 0, \"Persistence\"]\n",
    "df.drop('Train_time', inplace=True, axis=1)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55162855922bd606",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
