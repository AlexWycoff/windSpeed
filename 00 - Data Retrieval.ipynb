{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "583cbfc2760c7a2c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Data Retrieval Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8017033895ab857e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "In this notebook, we use code provided by the NREL to request and handle the necessary data and metadata from the NOW-23 Great Lakes dataset. The code provided by the NREL has been modified to make the automatic querying and concatenation of data possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8f7f9df88c9b83",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv, dotenv_values\n",
    "\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"API_KEY\")\n",
    "EMAIL = os.getenv(\"EMAIL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2829b48733d20c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# The function used to generate the 100 random site ids: \n",
    "# np.random.randint(0, 388080, (100))\n",
    "\n",
    "site_ids = pd.read_csv(\"site ids\", index_col=0)\n",
    "site_ids = [str(site_ids.iloc[x, 0]) for x in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "BASE_URL = \"https://developer.nrel.gov/api/wind-toolkit/v2/wind/offshore-great-lakes-download.csv?\"\n",
    "POINTS = site_ids\n",
    "YEARS = ['2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020']\n",
    "\n",
    "# This code is provided by NREL to request and handle data\n",
    "# Some modifications have been made so that csv requests can be made and appended automatically\n",
    "\n",
    "def main():\n",
    "    input_data = {\n",
    "        'attributes': 'temperature_80m,turbulent_kinetic_energy_80m,winddirection_80m,windspeed_80m',\n",
    "        'interval': '60',\n",
    "\n",
    "        'api_key': API_KEY,\n",
    "        'email': EMAIL,\n",
    "    }\n",
    "    \n",
    "    for id, location_ids in enumerate(POINTS):\n",
    "\n",
    "        print(f'Making request for point group {id + 1} of {len(POINTS)}...')\n",
    "        for name in YEARS:\n",
    "            \n",
    "            input_data['names'] = name\n",
    "            input_data['location_ids'] = location_ids\n",
    "\n",
    "            if '.csv' in BASE_URL:\n",
    "                url = BASE_URL + urllib.parse.urlencode(input_data, True)\n",
    "                data = pd.read_csv(url, on_bad_lines='skip', skiprows=2)\n",
    "                \n",
    "                print(f'Response data for year {name} and site {location_ids}')\n",
    "\n",
    "                file_loc = r\"Data\\NOW-23 Great Lakes [2000-2020] 60min/\" + str(location_ids + \".csv\")\n",
    "                \n",
    "                if name == '2000':\n",
    "                    data.to_csv(file_loc)\n",
    "                else:\n",
    "                    pd.concat([pd.read_csv(file_loc, index_col=0), data]).to_csv(file_loc)\n",
    "            else:\n",
    "                headers = {\n",
    "                    'x-api-key': API_KEY\n",
    "                }\n",
    "                data = get_response_json_and_handle_errors(requests.post(BASE_URL, input_data, headers=headers))\n",
    "                download_url = data['outputs']['downloadUrl']\n",
    "                # You can do with what you will the download url\n",
    "                print(data['outputs']['message'])\n",
    "                print(f\"Data can be downloaded from this url when ready: {download_url}\")\n",
    "\n",
    "                data = pd.read_csv(download_url)\n",
    "\n",
    "                # Delay for 1 second to prevent rate limiting\n",
    "                time.sleep(1)\n",
    "            print(f'Processed')\n",
    "\n",
    "\n",
    "def get_response_json_and_handle_errors(response: requests.Response) -> dict:\n",
    "    \"\"\"Takes the given response and handles any errors, along with providing\n",
    "    the resulting json\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    response : requests.Response\n",
    "        The response object\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        The resulting json\n",
    "    \"\"\"\n",
    "    if response.status_code != 200:\n",
    "        print(f\"An error has occurred with the server or the request. The request response code/status: {response.status_code} {response.reason}\")\n",
    "        print(f\"The response body: {response.text}\")\n",
    "        exit(1)\n",
    "\n",
    "    try:\n",
    "        response_json = response.json()\n",
    "    except:\n",
    "        print(f\"The response couldn't be parsed as JSON, likely an issue with the server, here is the text: {response.text}\")\n",
    "        exit(1)\n",
    "\n",
    "    if len(response_json['errors']) > 0:\n",
    "        errors = '\\n'.join(response_json['errors'])\n",
    "        print(f\"The request errored out, here are the errors: {errors}\")\n",
    "        exit(1)\n",
    "    return response_json\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f331da055774a5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# This code is modified from the above cell to retrieve metadata for all randomly selected points and concatenate it\n",
    "\n",
    "BASE_URL = \"https://developer.nrel.gov/api/wind-toolkit/v2/wind/offshore-great-lakes-download.csv?\"\n",
    "POINTS = site_ids\n",
    "\n",
    "input_data = {\n",
    "    'attributes': 'temperature_80m,turbulent_kinetic_energy_80m,winddirection_80m,windspeed_80m',\n",
    "    'interval': '60',\n",
    "\n",
    "    'api_key': API_KEY,\n",
    "    'email': EMAIL,\n",
    "}\n",
    "\n",
    "for id, location_ids in enumerate(POINTS):\n",
    "\n",
    "    print(f'Making request for point group {id + 1} of {len(POINTS)}...')\n",
    "    for name in ['2000']:\n",
    "\n",
    "        input_data['names'] = name\n",
    "        input_data['location_ids'] = location_ids\n",
    "\n",
    "        if '.csv' in BASE_URL:\n",
    "            url = BASE_URL + urllib.parse.urlencode(input_data, True)\n",
    "            data = pd.read_csv(url, on_bad_lines='skip')\n",
    "\n",
    "            print(f'Response metadata for year {name} and site {location_ids}')\n",
    "            \n",
    "            file_loc = r\"Data\\NOW-23 Great Lakes [2000-2020] metadata.csv\"\n",
    "            if id == 0:\n",
    "                data.to_csv(file_loc)\n",
    "            else:\n",
    "                pd.concat([pd.read_csv(file_loc, index_col=0), data]).to_csv(file_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd519bf1f1cdf5d6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Finish preprocessing the data by computing the vector components of the wind\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "for filename in os.listdir(r'Data\\NOW-23 Great Lakes [2000-2020] 60min'):\n",
    "    print(f'{filename}')\n",
    "    data = pd.read_csv(r'Data\\NOW-23 Great Lakes [2000-2020] 60min/' + '102812.csv', index_col=0)\n",
    "    data.dropna(how='all', axis=1, inplace=True)\n",
    "    \n",
    "    # Add new columns for u and v components of the wind speed\n",
    "    u_components, v_components = list(), list()\n",
    "    for i in range(len(data)):\n",
    "        u_components.append(data.iloc[i, 8] * np.cos(data.iloc[i, 7] * np.pi / 180))\n",
    "        v_components.append(data.iloc[i, 8] * np.sin(data.iloc[i, 7] * np.pi / 180))\n",
    "    \n",
    "    data[\"u component\"] = pd.Series(u_components)\n",
    "    data[\"v component\"] = pd.Series(v_components)\n",
    "    \n",
    "    data.to_csv(r'Data\\NOW-23 Great Lakes [2000-2020] 60min/' + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca830a85c1bfa710",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa5cafe58bdc49e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
